[
  {
    "objectID": "projects/clustering-recruits/ClusteringRecruitsP1.html",
    "href": "projects/clustering-recruits/ClusteringRecruitsP1.html",
    "title": "Kara Niewoehner Projects",
    "section": "",
    "text": "Note\n\n\n\nLoad Libraries - This cell imports core Python packages for data handling, imputation, PCA, scaling, and visualization.\n\n\n\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\n\n\n\n\n\n\nNote\n\n\n\nLoad and Refine Data - We use KNNImputer instead of dropping rows to keep as much data as possible. - The isna() and sum() function is utilized to ensure no columns have NaN values\n\n\n\ncombine_knn_py_file = \"CompleteData.csv\"\ncol_impute = [\"Year\", \"HT\", \"WT\",\"40\",\"BJ\",\"5 10 5\",\"FLEX\", \"GPA\", \"Vert\"]\n\n\ncombine_knn_py = pd.read_csv(combine_knn_py_file)\n\nimputer = KNNImputer(n_neighbors=10)\ncombine_knn_py[col_impute] = imputer.fit_transform(combine_knn_py[col_impute])\n\nprint(combine_knn_py[col_impute].isna().sum())\n\nYear      0\nHT        0\nWT        0\n40        0\nBJ        0\n5 10 5    0\nFLEX      0\nGPA       0\nVert      0\ndtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\nDataset Summary - Displays key descriptive statistics for each numeric variable.\n- Used to verify data ranges and confirm that imputation was successful before applying scaling and PCA.\n\n\n\ncombine_knn_py.describe()\n\n\n\n\n\n\n\n\nYear\nHT\nWT\n40\nBJ\n5 10 5\nFLEX\nGPA\nVert\n\n\n\n\ncount\n543.000000\n543.000000\n543.000000\n543.000000\n543.000000\n543.000000\n543.000000\n543.000000\n543.000000\n\n\nmean\n2025.270350\n69.287753\n199.273554\n4.617540\n11.488562\n4.746517\n3.296501\n4.327902\n28.168936\n\n\nstd\n0.555005\n11.805654\n37.586046\n0.935849\n17.155088\n0.385607\n1.099432\n5.409156\n1.949983\n\n\nmin\n2025.000000\n0.000000\n4.500000\n1.630000\n0.000000\n2.460000\n1.000000\n2.500000\n17.850000\n\n\n25%\n2025.000000\n70.000000\n175.000000\n4.670000\n7.865000\n4.550000\n3.000000\n3.750000\n27.457500\n\n\n50%\n2025.000000\n71.950000\n192.000000\n4.830000\n8.580000\n4.720000\n3.000000\n3.920000\n27.965000\n\n\n75%\n2025.200000\n73.000000\n215.000000\n5.042500\n8.980000\n4.942500\n4.000000\n4.183000\n29.090000\n\n\nmax\n2028.000000\n77.000000\n359.000000\n7.370000\n122.000000\n6.770000\n5.000000\n93.700000\n37.950000\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStandardize and Apply PCA - Uses StandardScaler() to normalize each feature to mean = 0 and standard deviation = 1.\n- Applies Principal Component Analysis (PCA) to reduce dimensionality while preserving most of the data variance.\n\n\n\nscaler = StandardScaler()\nscaled_combine_knn_py = scaler.fit_transform(combine_knn_py[col_impute])\n\npca = PCA(svd_solver = \"full\")\npca_fit_py = \\\npca.fit_transform(scaled_combine_knn_py)\n\n\n\n\n\n\n\nNote\n\n\n\nCombine PCA Components - Converts PCA output into labeled components (PC1, PC2, …) and appends them to the cleaned dataset.\n- This allows visualization and clustering directly in the reduced PCA feature space.\n\n\n\npca_fit_py = pd.DataFrame(pca_fit_py)\npca_fit_py.columns = \\\n    [\"PC\" + str(x + 1) for x in range(len(pca_fit_py.columns))]\n\ncombine_knn_py = \\\n    pd.concat([combine_knn_py, pca_fit_py], axis = 1)\n\n\n\n\n\n\n\nNote\n\n\n\nVisualize PCA Results\n\nPlots PC1 vs. PC2 to visualize the structure of the data in reduced dimensions.\n\nHelps detect patterns, outliers, and potential natural groupings before clustering.\n\n\n\n\nsns.scatterplot(data=combine_knn_py,\n                x = \"PC1\",\n                y = \"PC2\");\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nColor by PC3\n\nAdds a third dimension by coloring points according to PC3 values.\n\nReveals variation not visible in the 2D projection, helping interpret hidden relationships.\n\n\n\n\nsns.scatterplot(data=combine_knn_py,\n                x = \"PC1\",\n                y = \"PC2\",\n                hue=\"Pos\");\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nColor by Position\n\nColors each player’s PCA position by their on-field position (e.g., WR, QB, DL).\n\nDemonstrates how athlete roles differ based on physical and performance metrics.\n\n\n\n\nsns.scatterplot(data=combine_knn_py,\n                x = \"PC1\",\n                y = \"PC2\",\n                hue=\"PC3\");\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nK-Means Clustering\n\nUses kmeans() from scipy.cluster.vq to group players into six clusters based on PC1 and PC2.\n\nAssigns each player a cluster label, identifying groups of similar athletic profiles.\n\n\n\n\nfrom scipy.cluster.vq import vq, kmeans\n\nk_means_fit_py = \\\n    kmeans(combine_knn_py[[\"PC1\", \"PC2\"]], 6, seed = 1234)\n\n\n\n\n\n\n\nNote\n\n\n\nInspect Cluster Composition\n\nFilters for a specific cluster and groups by position to examine the distribution of roles.\n\nCalculates the mean and count of height and weight to describe each cluster’s physical profile.\n\n\n\n\ncombine_knn_py[\"cluster\"] = \\\n    vq(combine_knn_py[[\"PC1\", \"PC2\"]], k_means_fit_py[0])[0]\n\ncombine_knn_py.head()\n\n\n\n\n\n\n\n\nLast Name\nFirst Name\nPos\nYear\nHT\nWT\n40\nBJ\n5 10 5\nFLEX\n...\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\ncluster\n\n\n\n\n0\nGaunt\nNoah\nWR\n2025.0\n70.0\n170.0\n4.83\n8.78\n4.61\n3.0\n...\n-0.552453\n-0.100760\n0.029240\n-0.040877\n-0.451315\n-0.644661\n-0.360993\n-0.292047\n-0.459073\n0\n\n\n1\nCloninger\nLandon\nQB\n2025.0\n70.0\n168.0\n4.99\n9.05\n4.65\n5.0\n...\n-0.485666\n-0.942726\n-0.888733\n0.174139\n-0.426055\n-0.734481\n0.460280\n0.748461\n-0.540194\n5\n\n\n2\nHarper\nStiles\nDB\n2026.0\n71.0\n162.0\n4.80\n9.28\n4.57\n4.0\n...\n-1.037367\n-0.010626\n-1.115687\n-0.056103\n0.588594\n-0.082192\n0.760329\n-0.405096\n-0.223217\n0\n\n\n3\nWhite\nJayden\nDB\n2026.0\n72.0\n182.0\n4.91\n9.13\n4.80\n4.0\n...\n-0.743077\n-0.048514\n-0.898093\n0.092975\n1.072665\n0.639700\n0.430569\n0.192460\n0.040215\n0\n\n\n4\nLUCAS\nJORDAN\nWR\n2025.0\n68.0\n178.0\n4.75\n8.60\n4.92\n2.0\n...\n-0.361857\n0.550326\n0.594156\n-0.071676\n-0.125390\n0.157293\n-1.066494\n-0.116240\n-0.544457\n0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummarize Clusters by Position\n\nAggregates height and weight statistics for every position within each cluster.\n\nProvides insight into how body composition varies across clusters and roles.\n\n\n\n\nprint(\n    combine_knn_py.query(\"cluster == 1\")\n    .groupby(\"Pos\")\n    .agg({\"HT\": [\"count\", \"mean\"], \"WT\": [\"count\", \"mean\"]})\n)\n\n       HT               WT            \n    count       mean count        mean\nPos                                   \nDL      1  70.000000     1  270.000000\nOL     42  73.455952    42  276.792857\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisualize Cluster Distributions\n\nCreates a faceted bar chart of position frequencies per cluster.\n\nEnables visual comparison of which positions dominate each group.\n\n\n\n\ncombine_knn_py_cluster = \\\n    combine_knn_py\\\n    .groupby([\"cluster\", \"Pos\"])\\\n    .agg({\"HT\": [\"count\", \"mean\"],\n          \"WT\": [\"mean\"]}\n        )\ncombine_knn_py_cluster.columns = \\\n    list(map(\"_\".join, combine_knn_py_cluster.columns))\n\ncombine_knn_py_cluster.reset_index(inplace=True)\n\ncombine_knn_py_cluster\\\n    .rename(columns={\"HT_count\": \"n\",\n                     \"HT_mean\": \"HT\",\n                     \"WT_mean\": \"WT\"},\n                     inplace=True)\n\ncombine_knn_py_cluster.cluster = \\\n    combine_knn_py_cluster.cluster.astype(str)\n\nsns.catplot(combine_knn_py_cluster, x = \"n\", y = \"Pos\", \n            col=\"cluster\", col_wrap = 3, kind = \"bar\");\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCluster Averages\n\nDisplays overall average height and weight by cluster.\n\nSummarizes the key physical distinctions between clusters.\n\n\n\n\ncombine_knn_py_cluster\\\n    .groupby(\"cluster\")\\\n    .agg({\"HT\": [\"mean\"], \"WT\": [\"mean\"]})\n\n\n\n\n\n\n\n\nHT\nWT\n\n\n\nmean\nmean\n\n\ncluster\n\n\n\n\n\n\n0\n70.979415\n185.944977\n\n\n1\n71.727976\n273.396429\n\n\n2\n67.923747\n165.918354\n\n\n3\n64.412908\n200.986471\n\n\n4\n72.161181\n211.998573\n\n\n5\n71.545923\n199.971237\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSave Model Pipeline\n\nBuilds a Pipeline for reproducible preprocessing:\nKNNImputer → StandardScaler → PCA → KMeans.\n\nSaves the entire model (pipe, feature_cols, Z_recruits, recruits_meta, kmeans) as recruit_model.joblib for later use in the Streamlit app.\n\n\n\n\nFEATURES = [\"HT\",\"WT\",\"40\",\"BJ\",\"5 10 5\",\"FLEX\",\"GPA\"] \n\nrecruits_df = pd.read_csv(\"CompleteData.csv\")\n\nrecruits_meta = recruits_df[[\"First Name\",\"Last Name\",\"Pos\"]].copy()\n\n\npipe = Pipeline([\n    (\"imputer\", KNNImputer(n_neighbors=10)),\n    (\"scaler\",  StandardScaler()),\n    (\"pca\",     PCA(n_components=5, svd_solver=\"full\"))\n])\n\nXr = recruits_df[FEATURES].apply(pd.to_numeric, errors=\"coerce\")\nZr = pipe.fit_transform(Xr)\n\n\nkmeans = KMeans(n_clusters=6, n_init=10, random_state=42).fit(Zr)\nrecruits_meta[\"cluster\"] = kmeans.labels_\n\n\njoblib.dump(\n    {\n        \"pipe\": pipe,\n        \"feature_cols\": FEATURES,\n        \"Z_recruits\": Zr,\n        \"recruits_meta\": recruits_meta.reset_index(drop=True),\n        \"kmeans\": kmeans\n    },\n    \"recruit_model.joblib\"\n)\nprint(\"Saved recruit_model.joblib\")\n\nSaved recruit_model.joblib"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my GitHub Page!",
    "section": "",
    "text": "Hello! I’m Kara and I’m currently a student-athlete at Davidson College studying Mathematics and Data Science. I have discovered a love of working with numbers and problem-solving through code, and my esteemed projects will be located on this page!\nAs a woman in STEM and in collegiate sport, I want other girls and women to be uplifted and encouraged to follow their passions and dreams, no matter the social barriers. With this, I have been able to find an intersection between my interest in problem-solving and quantitative reasoning with my passion for the empowerment of women. I have had the opportunity to research, refine, and visualize the data surrounding the gender wage gap and the progress that various legislative actions had made on gender equity. This project, “Gender Inequities through an Economic Lens”, was created with R in Jupyter Notebook."
  }
]